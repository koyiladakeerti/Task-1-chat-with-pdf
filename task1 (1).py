# -*- coding: utf-8 -*-
"""Task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q13tQO2Y6ce1gO119TDF0m3YEsp2R7Si
"""

!pip install PyMuPDF
!pip install pdfplumber
!pip install sentence-transformers
!pip install transformers
!pip install torch
!pip install faiss-cpu
!pip install pandas
!pip install numpy

from google.colab import files

# Upload the PDF file
uploaded = files.upload()

# Check the uploaded file name
for file_name in uploaded.keys():
    print(f'Uploaded file: {file_name}')

# Step 1: Import libraries
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import faiss
import pandas as pd
from google.colab import files

# Step 2: Upload the PDF file
uploaded = files.upload()

# Get the uploaded PDF file name
pdf_path = next(iter(uploaded))  # Get the first uploaded file

# Step 3: PDF Extraction
def extract_pdf_text(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page_num in range(doc.page_count):
        page = doc.load_page(page_num)
        text += page.get_text("text")
    return text

# Step 4: Chunking & Embedding
def chunk_and_embed(text):
    sentences = text.split('\n')
    model = SentenceTransformer('all-MiniLM-L6-v2')  # You can also use other Sentence-BERT models
    embeddings = model.encode(sentences)
    return sentences, embeddings

# Step 5: Storing in FAISS Vector Database
def store_embeddings(embeddings):
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index

# Step 6: Query Processing
def process_query(query, index, sentences, model):
    query_embedding = model.encode([query])
    distances, indices = index.search(query_embedding, k=5)  # Retrieve top 5 most relevant chunks
    retrieved_chunks = [sentences[i] for i in indices[0]]
    return retrieved_chunks

# Step 7: Generate Response using Hugging Face model
def generate_response(query, retrieved_chunks):
    context = "\n".join(retrieved_chunks)
    generator = pipeline('text-generation', model='gpt2')  # You can use other models like T5, GPT-J, etc.
    response = generator(f"Based on the following data, answer the user's query: {query}\n\n{context}", max_length=150)
    return response[0]['generated_text']

# Main pipeline
text = extract_pdf_text(pdf_path)
sentences, embeddings = chunk_and_embed(text)
index = store_embeddings(embeddings)

# Query handling
query = "What is the unemployment rate for a Master's degree?"
retrieved_chunks = process_query(query, index, sentences, SentenceTransformer('all-MiniLM-L6-v2'))
response = generate_response(query, retrieved_chunks)
print(response)

# Query handling
query = "What is the unemployment rate for a Bachelor's degree?"
retrieved_chunks = process_query(query, index, sentences, SentenceTransformer('all-MiniLM-L6-v2'))
response = generate_response(query, retrieved_chunks)
print(response)

# Install required libraries
!pip install PyMuPDF sentence-transformers faiss-cpu transformers pdfplumber google-colab

import fitz  # PyMuPDF
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline
from google.colab import files

# Step 1: Upload the PDF
uploaded = files.upload()

# Get the uploaded PDF file name
pdf_path = next(iter(uploaded))  # Get the first uploaded file

# Step 2: PDF Extraction (Extracting text from the uploaded PDF)
def extract_pdf_text(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page_num in range(doc.page_count):
        page = doc.load_page(page_num)
        text += page.get_text("text")  # Extract text from each page
    return text

# Extracting text from the PDF
pdf_text = extract_pdf_text(pdf_path)

# Step 3: Chunking and Embedding the PDF Text
def chunk_and_embed(text):
    sentences = text.split('\n')
    model = SentenceTransformer('all-MiniLM-L6-v2')  # Sentence Transformer model for embeddings
    embeddings = model.encode(sentences)
    return sentences, embeddings

sentences, embeddings = chunk_and_embed(pdf_text)

# Step 4: Storing embeddings in FAISS for efficient retrieval
def store_embeddings(embeddings):
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)  # L2 distance metric
    index.add(embeddings)
    return index

index = store_embeddings(embeddings)

# Step 5: Process Query and Search for Relevant Information
def process_query(query, index, sentences, model):
    query_embedding = model.encode([query])  # Convert query to embedding
    distances, indices = index.search(query_embedding, k=5)  # Retrieve top 5 relevant chunks
    retrieved_chunks = [sentences[i] for i in indices[0]]
    return retrieved_chunks

# Step 6: Generate a Response using Hugging Face Model
def generate_response(query, retrieved_chunks):
    context = "\n".join(retrieved_chunks)
    generator = pipeline('text-generation', model='gpt2')  # You can use other models like T5, GPT-J, etc.
    response = generator(f"Answer the following question based on the provided text: {query}\n\nContext: {context}", max_length=150)
    return response[0]['generated_text']

# Step 7: Chatbot Interface
def chat_with_pdf():
    print("Chat with PDF: Type 'exit' to quit.")
    while True:
        query = input("Ask a question: ")
        if query.lower() == 'exit':
            print("Exiting chat.")
            break

        retrieved_chunks = process_query(query, index, sentences, SentenceTransformer('all-MiniLM-L6-v2'))
        response = generate_response(query, retrieved_chunks)
        print(f"Answer: {response}")

# Start the chatbot
chat_with_pdf()